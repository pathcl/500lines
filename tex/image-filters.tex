\begin{aosachapter}{Making Your Own Image Filters}{s:image-filters}{Cate Huston}

\emph{Cate Huston is a developer and entrepreneur focused on mobile.
She's lived and worked in the UK, Australia, Canada, China and the
United States, as an engineer at Google, an Extreme Blue intern at IBM,
and a ski instructor. Cate speaks internationally on mobile development
and her writing has been published on sites as varied as Lifehacker, The
Daily Beast, The Eloquent Woman and Model View Culture. She co-curates
Technically Speaking, blogs at Accidentally in Code and is
\href{https://twitter.com/catehstn}{@catehstn} on Twitter.}

\aosasecti{A Story of a Brilliant Idea That Wasn't All That
Brilliant}\label{a-story-of-a-brilliant-idea-that-wasnt-all-that-brilliant}

In Chinese art, there is often a series of four paintings showing the
same place in different seasons. Color - the cool whites of winter, pale
hues of spring, lush greens of summer, and red and yellows of fall is
the differentiation. Sometime around 2011, I had what I thought was a
brilliant idea. I wanted to be able to visualize a photo series, as a
series of colors. I thought it would show travel, and progression
through the seasons.

I didn't know how to calculate the dominant color from an image, and I
thought about scaling the image down to a 1x1 square and seeing what was
left, but that seemed like cheating. I knew how I wanted to display it
though, in a layout called the
\href{http://www.catehuston.com/applets/Sunflower/index.html}{Sunflower
layout}. It's the most efficient way to layout circles.

I left this project for years, distracted by work, life, travel, talks.
Eventually I returned to it, and figured out how to calculate the
dominant color and finally
\href{http://www.catehuston.com/blog/2013/09/02/visualising-a-photo-series/}{finished
my visualization}. And that is when I discovered that this idea wasn't,
in fact, brilliant. Because the progression wasn't as clear as I hoped,
the dominant color extracted wasn't generally the most appealing shade,
the creation took a long time (a couple of seconds per image), and
required hundreds of images to make something cool
\aosafigref{500.imagefilters.sunflower}.

\aosafigure[240pt]{image-filters-images/sunflower.jpg}{Sunflower layout}{500l.imagefilters.sunflower}

You might think this would be discouraging, but by the time I had got to
this point I had learned so many things that hadn't come my way before -
about color spaces, pixel manipulation, and I had started making these
cool partially colored images, of the kind you find on postcards of
London - the red bus, or phone booth, but everything else is in
gray-scale.

I used a framework called \href{https://processing.org/}{Processing}
because I was familiar with it from developing programming curricula,
and because I knew it made it really easy to create visual applications.
It's a tool originally designed for artists, so it abstracts away much
of the boilerplate. It allowed me to play, and to experiment.

University, and later work, had filled up my time with other people's
ideas and priorities. Part of ``finishing'' this project was learning
how to carve out time to make progress on my own ideas in the time that
was left to me. I calculated this to be about 4 hours of good mental
time a week. A tool that allowed me to move faster was therefore really
helpful, even necessary. Although it came with it's own set of problems,
especially around writing tests. I felt thorough tests were especially
important for validating how it was working, and for making it easier to
pick up and resume a project that was often on ice for weeks, even
months at a time. Tests (and blogposts!) formed the documentation for
this project. I could leave failing tests to document what should happen
that I hadn't quite figured out yet, make changes with more confidence
that if I changed something that I had forgotten was critical, the tests
would remind me.

This chapter will cover some details about Processing, talk you through
color spaces, decomposing an image into pixels and manipulating them,
and unit testing something that wasn't designed with testing in mind.
But I hope it will also prompt you to go and make some progress on
whatever idea you haven't made time for lately, because even if your
idea turns out to be as terrible as mine was, you may make something
cool, and learn something fascinating in the process.

\aosasecti{The App}\label{the-app}

This chapter will show you how to create your own image filter
application using Processing (a programming language and development
environment built on Java, used as a tool for artists to create with),
where you can load in your digital images and manipulate them yourself
using filters that you create. We'll cover aspects of color, setting up
the application in Processing, some of the features of Processing, how
to create color filters (mimicking what used to be used in old-fashioned
photography) and also a special kind of filter that can only be done
digitally - extracting the dominant hue from an image, and showing or
hiding it, to create eerie partially colored images.

We'll also be adding a thorough test suite, and covering how to handle
some of the limitations of Processing when it comes to testability.

\aosasecti{Background}\label{background}

Today we can take a photo, manipulate it, and share it with all our
friends in a matter of seconds. However, a long long time ago (in
digital terms anyway), it used to be a process that would take weeks.

We would take the picture, then when we had used a whole roll of film,
we would take it in to be developed (often at the pharmacy), pick it up
some days later, and then discover that there was something wrong with
many of the pictures - hand not steady enough? Random person/thing that
we didn't remember seeing at the time. Of course by then it was too late
to remedy it.

Then next time we had friends over, we could show them our carefully
curated album of that trip we took, or alternatively, just tip the
pictures out of the shoebox we were keeping them in, onto the coffee
table.

The process that would turn the film into the picture was one that most
people didn't understand. Light was a problem, so you had to be careful
with the film. There was some process featuring darkened rooms and
chemicals that they sometimes showed bits of in films or on TV.

Which actually sounds familiar. Few people understand how we get from
the point and click on our smartphone camera to an image on instagram.
But actually there are many similarities.

\aosasectii{Photographs, the Old Way}\label{photographs-the-old-way}

Photographs are created by the effect of light on a light sensitive
surface. Photographic film is covered in silver halide crystals (extra
layers are used to create color photographs - for simplicity let's just
stick to black-and-white photography here.)

When talking an old fashioned style photograph - with film - the light
hits the film according to what you're pointing at, and the crystals at
those points are changed in varying degrees - according to the amount of
light. Then, the
\href{http://photography.tutsplus.com/tutorials/step-by-step-guide-to-developing-black-and-white-t-max-film-{}-photo-2580}{development
process} converts the silver salts to metallic silver, creating the
negative. The negative has light and dark areas of the image inverted to
their opposite. Once the negatives have been developed, there is another
series of steps that reverse the image back and print it.

\aosasectii{Photographs, the Digital
Way}\label{photographs-the-digital-way}

When taking pictures using our smartphones or digital cameras, there is
no film. There is something called an Active Pixel Sensor which
functions in a similar way. Where we used to have silver crystals, now
we have pixels - tiny squares (in fact, pixel is short for ``picture
element''). Digital images are made up of pixels, and the higher the
resolution the more pixels there are. This is why low-resolution images
are described as ``pixelated'' - you can start to see the squares. These
pixels are just stored in an array, which the number in each array
``box'' containing the color.

In \aosafigref{500l.imagefilters.animals}, we see some blow up animals
taken at MoMA in NYC at high resolution.
\aosafigref{500l.imagefilters.pixelanimals} is the same image blown-up,
but with just 24 x 32 pixels.

\aosafigure[240pt]{image-filters-images/animals.jpg}{Blow-up animals at MoMA NY}{500l.imagefilters.animals}

\aosafigure[240pt]{image-filters-images/pixelanimals.jpg}{Blow-up animals, blown-up}{500l.imagefilters.pixelanimals}

See how it is so blurry? The lines of the animals aren't as smooth? We
call that \emph{pixelation}, which means the image is too big for the
number of pixels it contains, and the squares become visible. Here we
can use it to get a better sense of an image made up of squares of
color.

What do these pixels look like? If we print out the colors of some of
the pixels in the middle (10,10 to 10,14) using the handy
\texttt{Integer.toHexString} in Java, we get hex colors:

\begin{verbatim}
FFE8B1
FFFAC4
FFFCC3
FFFCC2
FFF5B7
\end{verbatim}

Hex colors are 6 characters long. The first two are the red value, the
second two the green value, and the third two the blue value. Sometimes
there are an extra two characters which are the alpha value. In this
case \texttt{FFFAC4} means:

\begin{aosaitemize}

\item
  red = FF (hex) = 255 (base 10)
\item
  green = FA (hex) = 250 (base 10)
\item
  blue = C4 (hex) = 196 (base 10)
\end{aosaitemize}

\aosasecti{Running The App}\label{running-the-app}

In \aosafigref{500l.imagefilters.app}, we have a picture of our app
running. It's very much developer-designed, I know, but we only have 500
lines of Java here so something had to suffer! You can see the list of
commands on the right. Some things we can do:

\begin{aosaitemize}

\item
  Adjust the RGB filters.
\item
  Adjust the ``hue tolerance''.
\item
  Set the dominant hue filters, to either show or hide the dominant hue.
\item
  Apply our current setting (it is infeasible to run this every key
  press).
\item
  Reset the image.
\item
  Save the image we have made.
\end{aosaitemize}

\aosafigure[240pt]{image-filters-images/app.jpg}{The App}{500l.imagefilters.app}

We're using Processing, as it makes it really simple to create a little
application, and do image manipulation. Processing is an IDE (Integrated
Development Environment) and a set of libraries to make visual
applications. It was originally created as a way for designers and
artists to create digital apps, so it has a very visual focus.

We'll focus on the Java-based version although Processing has now been
ported to other languages, including Javascript, which is awesome if you
want to upload your apps to the internet.

For this tutorial, I use it in Eclipse by adding \texttt{core.jar} to my
build path. If you want, you can use the Processing IDE, which removes
the need for a lot of boilerplate Java code. If you later want to port
it over to Processing.js and upload it online, you need to replace the
file chooser with something else.

There are detailed instructions with screenshots in the project's
\href{https://github.com/aosabook/500lines/blob/master/image-filters/SETUP.MD}{repository}.
If you are familiar with Eclipse and Java already you may not need them.

\aosasecti{Processing Basics}\label{processing-basics}

\aosasectii{Size and Color}\label{size-and-color}

We don't want our app to be a tiny grey window, so the two essential
methods that we will start by overriding (implementing in our own class,
instead of using the default implementation in the superclass, in this
case \texttt{PApplet}) are
\href{http://processing.org/reference/setup_.html}{\texttt{setup()}},
and \href{http://processing.org/reference/draw_.html}{\texttt{draw()}}.
\texttt{setup()} is only called when the app starts, and is where we do
things like set the size. \texttt{draw()} is called for every animation,
or after some action can be triggered by calling \texttt{redraw()} (as
covered in the Processing Documentation, \texttt{draw()} should not be
called explicitly).

Processing is designed to work nicely to create animated sketches, but
in this case we don't want animation\footnote{If we wanted to create an
  animated sketch we would not call \texttt{noLoop()} (or, if we wanted
  to start animating later, we would call \texttt{loop()}). The
  frequency of the animation is determined by \texttt{frameRate()}.}, we
want to respond to key presses. To prevent animation (this would be a
drag on performance) we will want to call
\href{http://www.processing.org/reference/noLoop_.html}{\texttt{noLoop()}}
from setup. This means that \texttt{draw()} will only be called
immediately after \texttt{setup()}, and whenever we call
\texttt{redraw()}.

\begin{verbatim}
    private static final int WIDTH = 360;
    private static final int HEIGHT = 240;

    public void setup() {
        noLoop();

        // Set up the view.
        size(WIDTH, HEIGHT);
        background(0);
    }
        
    public void draw() {
        background(0);
    }
\end{verbatim}

These don't really do much yet, but run the app again adjusting the
constants in \texttt{WIDTH} and \texttt{HEIGHT} to see different sizes.

\texttt{background(0)} specifies a black background. Try changing the
number passed into \texttt{background()} and see what happens - it's the
alpha value, and so if you only pass one number in, it is always
greyscale. Alternatively, you can call
\texttt{background(int r, int g, int b)}.

\aosasectii{PImage}\label{pimage}

The \href{http://processing.org/reference/PImage.html}{PImage object} is
the Processing object that represents an image. We're going to be using
this a lot, so it's worth reading through the documentation. It has
three fields (\aosatblref{500l.imagefilters.pimagefields}) as well as
some methods that we will use
(\aosatblref{500l.imagefilters.pimagemethods}).

\begin{table}
\centering
{\footnotesize
\rowcolors{2}{TableOdd}{TableEven}
\begin{tabular}{ll}
\hline
pixels[] & Array containing the color of every pixel in the image \\
width & Image width in pixels \\
height & Image height in pixels \\
\hline
\end{tabular}
}
\caption{PImage fields}
\label{500l.imagefilters.pimagefields}
\end{table}

\begin{table}
\centering
{\footnotesize
\rowcolors{2}{TableOdd}{TableEven}
\begin{tabular}{ll}
\hline
loadPixels & Loads the pixel data for the image into its `pixels[]` array \\
updatePixels & Updates the image with the data in its `pixels[]` array \\
resize & Changes the size of an image to a new width and height \\
get & Reads the color of any pixel or grabs a rectangle of pixels \\
set & Writes a color to any pixel or writes an image into another \\
save & Saves the image to a TIFF, TARGA, PNG, or JPEG file \\
\hline
\end{tabular}
}
\caption{PImage methods}
\label{500l.imagefilters.pimagemethods}
\end{table}

\aosasectii{File Chooser}\label{file-chooser}

Processing handles most of this, we just need to call
\href{http://www.processing.org/reference/selectInput_.html}{\texttt{selectInput()}},
and implement a callback (which must be public).

To people familiar with Java this might seem odd, a listener or a lambda
expression might make more sense. However as Processing was developed as
a tool for artists, for the most part the necessity for these things has
been abstracted away by the language to keep it unintimidating. This is
a choice the designers made - to prioritize simplicity and being
unintimidating over power and flexibility. If you use the stripped down
Processing editor, rather than Processing as a library in Eclipse you
don't even need to define class names!

Other language designers with different target audiences make different
choices, as they should. For example if we consider Haskell, a purely
functional language, that purity of functional language paradigms is
prioritised over everything else. This makes it a better tool for
mathematical problems than anything requiring IO.

\begin{verbatim}
// Called on key press.
private void chooseFile() {
    // Choose the file.
    selectInput("Select a file to process:", "fileSelected");
}

public void fileSelected(File file) {
    if (file == null) {
        println("User hit cancel.");
    } else {
        // save the image
        redraw(); // update the display
    }
}
\end{verbatim}

\aosasectii{Responding To Key Presses}\label{responding-to-key-presses}

Normally in Java doing this requires adding listeners and implementing
anonymous functions. However like the file chooser, Processing handles a
lot of this for us. We just need to implement
\href{https://www.processing.org/reference/keyPressed_.html}{\texttt{keyPressed()}}.

\begin{verbatim}
public void keyPressed() {
    print(“key pressed: ” + key);
}
\end{verbatim}

If you run the app again, every time you press a key it will output it
to the console. Later, you'll want to do different things depending on
what key was pressed, and to do this you just switch on the key value
(this exists in the \texttt{PApplet} superclass, and contains the last
key pressed).

\aosasecti{Writing Tests}\label{writing-tests}

This app doesn't do a lot yet, but we can already see number of places
where things can go wrong, for example triggering the wrong action with
key presses. As we add complexity, we add more potential problems, such
as updating the image state incorrectly, or miscalculations of the pixel
colors after applying a filter. I also just (some think weirdly) enjoy
writing unit tests. Whilst some people seem to think of testing as a
thing that delays checking code in, I see tests as my \#1 debugging
tool, and an opportunity to deeply understand what is going on in my
code.

I adore Processing, but as covered above it's designed as a tool for
artists to create visual applications, and in this maybe unit testing
isn't a huge concern. It's clear it isn't written for testability, in
fact it's written in such a way that makes it untestable, as is. Part of
this is because it hides complexity, some of that hidden complexity is
really useful in writing unit tests. The use of static and final methods
make it much harder to use mocks (objects that record interaction and
allow you to fake part of your system to verify another part is behaving
correctly), which rely on the ability to subclass.

We might start a greenfield project with great intentions to do Test
Driven Development (TDD) and achieve perfect test coverage, but in
reality we are usually looking at a mass of code written by various and
assorted people and trying to figure out what it is supposed to be
doing, and how and why it is going wrong. Then maybe we don't write
perfect tests, but writing tests at all will help us navigate this
situation, document what is happening and move forward.

To do that we create ``seams'' that will allow us to break something up
from it's amorphous mass of tangled pieces and verify. To do this, we
will sometimes create wrapper classes that can be mocked. These do
nothing more than hold a collection of similar methods, or forward calls
on to another object that can not be mocked (due to final or static
methods), and as such they are very dull to write, but key to creating
seams and making the code testable.

For tests, as I was working in Java with Processing as a library, I used
JUnit. For mocking, I used Mockito. You can download
\href{https://code.google.com/p/mockito/downloads/list}{mockito} and add
the jar to your buildpath in the same way you added \texttt{core.jar}. I
created two helper classes that make it possible to mock and test the
app (otherwise we can't test behavior involving \texttt{PImage} or
\texttt{PApplet} methods).

\texttt{IFAImage} is a thin wrapper around PImage.
\texttt{PixelColorHelper} is a wrapper around applet pixel color
methods. These wrappers call the final, and static methods, but the
caller methods are neither final nor static themselves - this allows
them to be mocked. These are deliberately lightweight, and we could have
gone further, however this was sufficient to address the major problem
of testability when using Processing - static, and final methods. The
goal here was to make an app after all - not a unit testing framework
for Processing!

A class called \texttt{ImageState} forms the ``model'' of this
application, removing as much logic from the class extending
\texttt{PApplet} as possible, for better testability. It also makes for
a cleaner design and separation of concerns - the \texttt{App} controls
the interactions and the UI, not the details of the image manipulation.

\aosasecti{Do It Yourself Filters}\label{do-it-yourself-filters}

\aosasectii{RGB Filters}\label{rgb-filters}

Before we start writing more complicated pixel processing, we can start
with a short exercise that will get us comfortable doing pixel
manipulation . We'll create standard (red, green, blue) color filters
that will allow us to create the same effect as a colored plate over the
lens of a camera, only letting through light with enough red (or green,
or blue).

By applying different RGB filters to an image we can make it almost seem
like the seasons are different (remember the Chinese four seasons
paintings mentioned earlier?), depending on which colors are filtered
out and which are emphasized.

How do we do it?

\begin{aosaitemize}
\item
  Set the filter (you can combine red, green and blue filters as in the
  image earlier, I haven't in these examples so that the effect is
  clearer).
\item
  For each pixel in the image, check its RGB value.
\item
  If the red is less than the red filter, set the red to zero.
\item
  If the green is less than the green filter, set the green to zero.
\item
  If the blue is less than the blue filter, set the blue to zero.
\item
  Any pixel with insufficient of all of these colors will be black.
\end{aosaitemize}

Although our image is 2-dimensional, the pixels live in a 1-dimensional
array starting top left moving
\href{https://processing.org/tutorials/pixels/}{left to right, top to
bottom}. The array indices for a 4x4 image are shown in .

\begin{table}
\centering
{\footnotesize
\rowcolors{2}{TableOdd}{TableOdd}
\begin{tabular}{cccc}
\hline
0 & 1 & 2 & 3 \\
4 & 5 & 6 & 7 \\
8 & 9 & 10 & 11 \\
12 & 13 & 14 & 15 \\
\hline
\end{tabular}
}
\caption{Pixel indices for a 4x4 image}
\label{500l.imagefilters.pixelindices}
\end{table}

\begin{verbatim}
public void applyColorFilter(PApplet applet, IFAImage img, int minRed,
            int minGreen, int minBlue, int colorRange) {    
    img.loadPixels();
    int numberOfPixels = img.getPixels().length;
    for (int i = 0; i < numberOfPixels; i++) {
        int pixel = img.getPixel(i);
        float alpha = pixelColorHelper.alpha(applet, pixel);
        float red = pixelColorHelper.red(applet, pixel);
        float green = pixelColorHelper.green(applet, pixel);
        float blue = pixelColorHelper.blue(applet, pixel);
            
        red = (red >= minRed) ? red : 0;
        green = (green >= minGreen) ? green : 0;
        blue = (blue >= minBlue) ? blue : 0;
        
        image.setPixel(i, pixelColorHelper.color(applet, red, green, blue, alpha));
    }
}
\end{verbatim}

\aosasectii{Color}\label{color}

As our first example of an image filter showed, the concept and
representation of colors in a program is very important to understanding
how our filters work. To prepare ourselves for working on our next
filter, let's explore the concept of color a bit more.

You may not have known it at the time, but we were unwittingly using a
concept in the previous section called a ``color space'', which is way
of representing color digitally. Kids mixing paints learn that all
colors can be made from other colors, but things work slightly
differently in digital (less risk of being covered in paint!) but
similar. Processing makes it really easy to work with whatever color
space you want, but you need to know which one to pick, so it's
important to understand how they work.

\aosasectiii{RGB colors}\label{rgb-colors}

The color space that most programmers are familiar with is RGBA - red,
green, blue and alpha. In hexadecimal (base 16) the first two digits are
the amount of red the second two blue, the third two green, and the
final two (if they are there) are the alpha value. These values range
from 00 in base 16, 0 in base 10, through to FF, the equivalent of 255
in base 10. The alpha represents the opacity, where 0 is transparent,
and 100\% opaque.

\aosasectiii{HSB or HSV colors}\label{hsb-or-hsv-colors}

This color space is not quite as well known as RGB, the first number
represents the hue, the second the saturation (how intense the color
is), and the third number the brightness. The image at the top was
created by manipulating in this color space. The HSB color space can be
represented by drawing a cone. The hue is the position around the cone,
saturation the distance from the centre, and brightness the height (0
brightness is black).

\aosasectii{Extracting The Dominant Hue From an
Image}\label{extracting-the-dominant-hue-from-an-image}

Now we're comfortable with pixel manipulation, let's do something that
we could only do digitally. Digitally, we can manipulate it in a way
that isn't so uniform.

When I look through my stream of pictures, say on Flickr, I can see
themes emerging. The nighttime series from the boat I took at sunset
around Hong Kong harbour, the grey of North Korea, the lush greens of
Bali, the icy whites and pale blues of an Icelandic winter. Can we take
a picture and pull out that main color that dominates the scene?

It makes sense to use the HSB color space for this - we are interested
in the hue, not the saturation or brightness, when figuring out what the
main color is. It's possible to do this using RGB values, but more
difficult (we would have to compare all three values) and it would be
more sensitive to darkness. We can change to this colorspace using
\href{http://processing.org/reference/colorMode_.html}{colorMode}.

Having settled on this color space, it's simpler than it would have been
using RGB. We need to find the hue of each pixel, and figure out which
is most ``popular''. We probably don't want to be exact, we want to
group very similar hues together, and we can handle this using two
strategies.

Firstly we will round the decimals that come back to whole numbers, as
this makes it simple to determine which ``bucket'' we put each pixel in.
Secondly we can change the range of the hues. If we think back up to the
diagram above, we might think of hues as having 360 degrees (like a
circle). Processing uses 255 by default, which is the same as is typical
for RGB (255 is FF in hexadecimal). The higher the range we use, the
more distinct the hues in the picture will be. Using a smaller range
will allow us to group together similar hues. Using a 360 degree range,
it's unlikely that we will be able to tell the difference between a hue
of 224 and a hue of 225, as the difference is very small. If we make the
range one-third of that instead, 120, both these hues become 75 after
rounding.

We can change the range of hues using \texttt{colorMode}. If we call:

\begin{verbatim}
    colorMode(HSB, 120);
\end{verbatim}

We have just made our hue detection a bit less than half as exact as if
we used the 255 range. We also know that our hues will fall into 120
``buckets'', so we can simply go through our image, get the hue for a
pixel, and add one to the corresponding count in an array. This will be
order of $O(n)$, where $n$ is the number of pixels, as it requires
action on each one.

\begin{verbatim}
for(int px in pixels) {
  int hue = Math.round(hue(px));
  hues[hue]++;
}
\end{verbatim}

At the end we can print this hue to the screen, or display it next to
the picture.

FIXME Show some of the example images here in markdown.

Sometimes changing the size of the range has a significant effect, and
in other cases very little. This is because the higher the range is, the
more ``exact'' the matching is. In some cases, for example the picture
of the trees, there are many shades of green, so when we make the hues
too exact, we end up picking out the color of the sky.

Once we've extracted the ``dominant'' hue, we can choose to either show
or hide it in the image.

We can show the dominant hue with varying tolerance (ranges around it
that we will accept). Pixels that don't fall into this range can be
changed to grayscale, by setting the value based on the brightness. The
below examples show the dominant hue determined using a hue range of
320, and with varying tolerance. The tolerance is the amount either side
of the most popular hue that gets grouped together.

FIXME show one example ``keep dominant hue'' photo in markdown.

Alternatively, we can hide the dominant hue.

FIXME show one example ``hide dominant hue'' photo in markdown.

Each image requires a double pass (looking at each pixel twice), so on
images with a large number of pixels it can take a noticeable amount of
time.

\begin{verbatim}
public HSBColor getDominantHue(PApplet applet, IFAImage image, int hueRange) {
        image.loadPixels();
        int numberOfPixels = image.getPixels().length;
        int[] hues = new int[hueRange];
        float[] saturations = new float[hueRange];
        float[] brightnesses = new float[hueRange];

        for (int i = 0; i < numberOfPixels; i++) {
            int pixel = image.getPixel(i);
            int hue = Math.round(pixelColorHelper.hue(applet, pixel));
            float saturation = pixelColorHelper.saturation(applet, pixel);
            float brightness = pixelColorHelper.brightness(applet, pixel);
            hues[hue]++;
            saturations[hue] += saturation;
            brightnesses[hue] += brightness;
        }

        // Find the most common hue.
        int hueCount = hues[0];
        int hue = 0;
        for (int i = 1; i < hues.length; i++) {
            if (hues[i] > hueCount) {
                hueCount = hues[i];
                hue = i;
            }
        }

        // Return the color to display.
        float s = saturations[hue] / hueCount;
        float b = brightnesses[hue] / hueCount;
        return new HSBColor(hue, s, b);
    }


public void processImageForHue(PApplet applet, IFAImage image, int hueRange,
        int hueTolerance, boolean showHue) {
    applet.colorMode(PApplet.HSB, (hueRange - 1));
    image.loadPixels();
    int numberOfPixels = image.getPixels().length;
    HSBColor dominantHue = getDominantHue(applet, image, hueRange);
    // Manipulate photo, grayscale any pixel that isn't close to that hue.
    float lower = dominantHue.h - hueTolerance;
    float upper = dominantHue.h + hueTolerance;
    for (int i = 0; i < numberOfPixels; i++) {
        int pixel = image.getPixel(i);
        float hue = pixelColorHelper.hue(applet, pixel);
        if (hueInRange(hue, hueRange, lower, upper) == showHue) {
            float brightness = pixelColorHelper.brightness(applet, pixel);
            image.setPixel(i, pixelColorHelper.color(applet, brightness));
        }
    }
    image.updatePixels();
}
\end{verbatim}

\aosasectii{Combining Filters}\label{combining-filters}

With the UI as it is, the user can combine the red, green, and blue
filters together. If they combine the dominant hue filters with the red,
green, and blue filters the results can sometimes be a little unexpected
- because of changing the color spaces.

Processing has some
\href{https://www.processing.org/reference/filter_.html}{built in
methods} that support the manipulation of images, for example
\texttt{invert} and \texttt{blur}.

To achieve effects like sharpening, or blurring, or sepia ourselves we
apply matrices. For every pixel of the image, take the sum of products
where each product is the color value of the current pixel or a neighbor
of it, with the corresponding value of the
\href{http://lodev.org/cgtutor/filtering.html}{filter matrix}. There are
some special matrices of specific values that sharpen images.

\aosasecti{Architecture}\label{architecture}

There are three main components to the app.

\aosasectii{The App}\label{the-app-1}

Consists of one file: \texttt{ImageFilterApp.java}. This extends
\texttt{PApplet} (the Processing app superclass) and handles layout,
user interaction etc. This class is the hardest to test, so we want to
keep it as small as possible.

\aosasectii{Color}\label{color-1}

Consists of two files: \texttt{ColorHelper.java}, which is where all the
image processing and filtering takes place. And
\texttt{PixelColorHelper.java} which abstracts out final
\texttt{PApplet} methods for pixel colors for testability.

\aosasectii{Model}\label{model}

Consists of three files. \texttt{HSBColor.java} which is a simple
container for HSB colors (consisting of hue, saturation, and
brightness). \texttt{IFAImage} which is a wrapper around \texttt{PImage}
for testability (\texttt{PImage} contains a number of final methods
which cannot be mocked). Finally \texttt{ImageState.java} is the object
describing the state of the image - what level of filters should be
applied, and which, and handles loading the image (note: the image needs
to be reloaded whenever color filters are adjusted down, and whenever
the dominant hue is recalculated. For clarity, we just reload each time
the image is processed).

FIXME Show Architecture Diagram here

\aosasectii{Wrapper Classes and Tests}\label{wrapper-classes-and-tests}

Briefly mentioned above, there are two wrapper classes
(\texttt{IFAImage} and \texttt{PixelColorHelper}) that wrap library
methods for testability. This is because in Java, final methods are
methods that cannot be overridden or hidden by subclasses, which means
they cannot be mocked.

\texttt{PixelColorHelper} wraps methods on the applet. This means we
need to pass the applet in to each method call (we could alternatively
make it a field and set it on initialization).

\begin{verbatim}
package com.catehuston.imagefilter.color;

import processing.core.PApplet;

public class PixelColorHelper {

    public float alpha(PApplet applet, int pixel) {
        return applet.alpha(pixel);
    }

    public float blue(PApplet applet, int pixel) {
        return applet.blue(pixel);
    }

    public float brightness(PApplet applet, int pixel) {
        return applet.brightness(pixel);
    }

    public int color(PApplet applet, float greyscale) {
        return applet.color(greyscale);
    }

    public int color(PApplet applet, float red, float green, float blue,
           float alpha) {
        return applet.color(red, green, blue, alpha);
    }

    public float green(PApplet applet, int pixel) {
        return applet.green(pixel);
    }

    public float hue(PApplet applet, int pixel) {
        return applet.hue(pixel);
    }

    public float red(PApplet applet, int pixel) {
        return applet.red(pixel);
    }

    public float saturation(PApplet applet, int pixel) {
        return applet.saturation(pixel);
    }
}
\end{verbatim}

\texttt{IFAImage} is a wrapper around \texttt{PImage}, so in our app we
don't initialize a \texttt{PImage}, but rather an \texttt{IFAImage}
instead. Although we do have to expose the \texttt{PImage} so that it
can be rendered.

\begin{verbatim}
package com.catehuston.imagefilter.model;

import processing.core.PApplet;
import processing.core.PImage;

public class IFAImage {

    private PImage image;

    public IFAImage() {
        image = null;
    }

    public PImage image() {
        return image;
    }

    public void update(PApplet applet, String filepath) {
        image = null;
        image = applet.loadImage(filepath);
    }

    // Wrapped methods from PImage.

    public int getHeight() {
        return image.height;
    }

    public int getPixel(int px) {
        return image.pixels[px];
    }

    public int[] getPixels() {
        return image.pixels;
    }

    public int getWidth() {
        return image.width;
    }

    public void loadPixels() {
        image.loadPixels();
    }

    public void resize(int width, int height) {
        image.resize(width, height);
    }

    public void save(String filepath) {
        image.save(filepath);
    }

    public void setPixel(int px, int color) {
        image.pixels[px] = color;
    }

    public void updatePixels() {
        image.updatePixels();
    }
}
\end{verbatim}

Finally, we have our simple container class, \texttt{HSBColor}. Note
that it is immutable (once created, it cannot be changed). Immutable
objects are better for thread safety (something we have no need of
here!) but are also easier to understand and reason about. In general, I
tend to make simple model classes immutable unless I find a good reason
for them not to be, and in this case no such reason arose.

Some of you may know that there are already classes representing color
in
\href{https://www.processing.org/reference/color_datatype.html}{Processing}
and in
\href{https://docs.oracle.com/javase/7/docs/api/java/awt/Color.html}{Java
itself}. Without going too much into the details of these, both of them
are more focused on RGB color, and the Java class in particular adds way
more complexity than we need. We would probably be OK if we did want to
use Java's awt.Color, however
\href{http://processing.org/reference/javadoc/core/processing/core/PApplet.html}{awt
GUI components cannot be used in Processing} so for our purposes
creating this simple container class to just hold these bits of data we
need is easiest.

\begin{verbatim}
package com.catehuston.imagefilter.model;

public class HSBColor {

    public final float h;
    public final float s;
    public final float b;

    public HSBColor(float h, float s, float b) {
        this.h = h;
        this.s = s;
        this.b = b;
    }
}
\end{verbatim}

\aosasectii{ColorHelper and Associated
Tests}\label{colorhelper-and-associated-tests}

\texttt{ColorHelper} is where all the image manipulation lives. The
methods in this class could be static if not for needing a
\texttt{PixelColorHelper}. Although we won't get into the debate as to
the merits of static methods here!

\begin{verbatim}
package com.catehuston.imagefilter.color;

import processing.core.PApplet;

import com.catehuston.imagefilter.model.HSBColor;
import com.catehuston.imagefilter.model.IFAImage;

public class ColorHelper {

    private final PixelColorHelper pixelColorHelper;

    public ColorHelper(PixelColorHelper pixelColorHelper) {
        this.pixelColorHelper = pixelColorHelper;
    }

    public boolean hueInRange(float hue, int hueRange, float lower, float upper) {
        // Need to compensate for it being circular - can go around.
        if (lower < 0) {
            lower += hueRange;
        }
        if (upper > hueRange) {
            upper -= hueRange;
        }
        if (lower < upper) {
            return hue < upper && hue > lower;
        } else {
            return hue < upper || hue > lower;
        }
    }

    public HSBColor getDominantHue(PApplet applet, IFAImage image, int hueRange) {
        image.loadPixels();
        int numberOfPixels = image.getPixels().length;
        int[] hues = new int[hueRange];
        float[] saturations = new float[hueRange];
        float[] brightnesses = new float[hueRange];

        for (int i = 0; i < numberOfPixels; i++) {
            int pixel = image.getPixel(i);
            int hue = Math.round(pixelColorHelper.hue(applet, pixel));
            float saturation = pixelColorHelper.saturation(applet, pixel);
            float brightness = pixelColorHelper.brightness(applet, pixel);
            hues[hue]++;
            saturations[hue] += saturation;
            brightnesses[hue] += brightness;
        }

        // Find the most common hue.
        int hueCount = hues[0];
        int hue = 0;
        for (int i = 1; i < hues.length; i++) {
            if (hues[i] > hueCount) {
                hueCount = hues[i];
                hue = i;
            }
        }

        // Return the color to display.
        float s = saturations[hue] / hueCount;
        float b = brightnesses[hue] / hueCount;
        return new HSBColor(hue, s, b);
    }

    public void processImageForHue(PApplet applet, IFAImage image, int hueRange,
            int hueTolerance, boolean showHue) {
        applet.colorMode(PApplet.HSB, (hueRange - 1));
        image.loadPixels();
        int numberOfPixels = image.getPixels().length;
        HSBColor dominantHue = getDominantHue(applet, image, hueRange);
        // Manipulate photo, grayscale any pixel that isn't close to that hue.
        float lower = dominantHue.h - hueTolerance;
        float upper = dominantHue.h + hueTolerance;
        for (int i = 0; i < numberOfPixels; i++) {
            int pixel = image.getPixel(i);
            float hue = pixelColorHelper.hue(applet, pixel);
            if (hueInRange(hue, hueRange, lower, upper) == showHue) {
                float brightness = pixelColorHelper.brightness(applet, pixel);
                image.setPixel(i, pixelColorHelper.color(applet, brightness));
            }
        }
        image.updatePixels();
    }

    public void applyColorFilter(PApplet applet, IFAImage image, int minRed,
            int minGreen, int minBlue, int colorRange) {
        applet.colorMode(PApplet.RGB, colorRange);
        image.loadPixels();
        int numberOfPixels = image.getPixels().length;
        for (int i = 0; i < numberOfPixels; i++) {
            int pixel = image.getPixel(i);
            float alpha = pixelColorHelper.alpha(applet, pixel);
            float red = pixelColorHelper.red(applet, pixel);
            float green = pixelColorHelper.green(applet, pixel);
            float blue = pixelColorHelper.blue(applet, pixel);

            red = (red >= minRed) ? red : 0;
            green = (green >= minGreen) ? green : 0;
            blue = (blue >= minBlue) ? blue : 0;

            image.setPixel(i, pixelColorHelper.color(applet, red, green, blue,
                alpha));
        }
    }
}
\end{verbatim}

Clearly we can't test this with whole images. Instead we can mock the
images and make them return an array of pixels - in this case, 5. This
allows us to verify that the behavior is as expected. Earlier we covered
the concept of mock objects, and here we see their use. We are using
\href{http://docs.mockito.googlecode.com/hg/org/mockito/Mockito.html}{Mockito}
as our mock object framework.

To create a mock we use the \texttt{@Mock} annotation on an instance
variable, and the \texttt{MockitoJUnitRunner} will mock it for us at
runtime.

To stub (set the behavior of) a method, we use
\texttt{when(mock.methodCall()).thenReturn(value)}.

To verify a method was called, we use
\texttt{verify(mock.methodCall())}.

We'll show a few example test cases here; if you'd like to see the rest,
visit the source folder for this project in the
\href{https://github.com/aosabook/500lines/tree/master/image-filters}{\emph{500
Lines or Less} GitHub repository}.

\begin{verbatim}
package com.catehuston.imagefilter.color;

/* ... Imports omitted ... */

@RunWith(MockitoJUnitRunner.class)
public class ColorHelperTest {

    @Mock PApplet applet;
    @Mock IFAImage image;
    @Mock PixelColorHelper pixelColorHelper;

    ColorHelper colorHelper;

    private static final int px1 = 1000;
    private static final int px2 = 1010;
    private static final int px3 = 1030;
    private static final int px4 = 1040;
    private static final int px5 = 1050;
    private static final int[] pixels = { px1, px2, px3, px4, px5 };

    @Before public void setUp() throws Exception {
        colorHelper = new ColorHelper(pixelColorHelper);
        when(image.getPixels()).thenReturn(pixels);
        setHsbValuesForPixel(0, px1, 30F, 5F, 10F);
        setHsbValuesForPixel(1, px2, 20F, 6F, 11F);
        setHsbValuesForPixel(2, px3, 30F, 7F, 12F);
        setHsbValuesForPixel(3, px4, 50F, 8F, 13F);
        setHsbValuesForPixel(4, px5, 30F, 9F, 14F);
    }

    private void setHsbValuesForPixel(int px, int color, float h, float s, float b) {
        when(image.getPixel(px)).thenReturn(color);
        when(pixelColorHelper.hue(applet, color)).thenReturn(h);
        when(pixelColorHelper.saturation(applet, color)).thenReturn(s);
        when(pixelColorHelper.brightness(applet, color)).thenReturn(b);
    }

    private void setRgbValuesForPixel(int px, int color, float r, float g, float b, 
            float alpha) {
        when(image.getPixel(px)).thenReturn(color);
        when(pixelColorHelper.red(applet, color)).thenReturn(r);
        when(pixelColorHelper.green(applet, color)).thenReturn(g);
        when(pixelColorHelper.blue(applet, color)).thenReturn(b);
        when(pixelColorHelper.alpha(applet, color)).thenReturn(alpha);
    }

    @Test public void testHsbColorFromImage() {
        HSBColor color = colorHelper.getDominantHue(applet, image, 100);
        verify(image).loadPixels();

        assertEquals(30F, color.h, 0);
        assertEquals(7F, color.s, 0);
        assertEquals(12F, color.b, 0);
    }

    @Test public void testProcessImageNoHue() {
        when(pixelColorHelper.color(applet, 11F)).thenReturn(11);
        when(pixelColorHelper.color(applet, 13F)).thenReturn(13);
        colorHelper.processImageForHue(applet, image, 60, 2, false);
        verify(applet).colorMode(PApplet.HSB, 59);
        verify(image, times(2)).loadPixels();
        verify(image).setPixel(1, 11);
        verify(image).setPixel(3, 13);
    }

    @Test public void testApplyColorFilter() {
        setRgbValuesForPixel(0, px1, 10F, 12F, 14F, 60F);
        setRgbValuesForPixel(1, px2, 20F, 22F, 24F, 70F);
        setRgbValuesForPixel(2, px3, 30F, 32F, 34F, 80F);
        setRgbValuesForPixel(3, px4, 40F, 42F, 44F, 90F);
        setRgbValuesForPixel(4, px5, 50F, 52F, 54F, 100F);

        when(pixelColorHelper.color(applet, 0F, 0F, 0F, 60F)).thenReturn(5);
        when(pixelColorHelper.color(applet, 20F, 0F, 0F, 70F)).thenReturn(15);
        when(pixelColorHelper.color(applet, 30F, 32F, 0F, 80F)).thenReturn(25);
        when(pixelColorHelper.color(applet, 40F, 42F, 44F, 90F)).thenReturn(35);
        when(pixelColorHelper.color(applet, 50F, 52F, 54F, 100F)).thenReturn(45);

        colorHelper.applyColorFilter(applet, image, 15, 25, 35, 100);
        verify(applet).colorMode(PApplet.RGB, 100);
        verify(image).loadPixels();

        verify(image).setPixel(0, 5);
        verify(image).setPixel(1, 15);
        verify(image).setPixel(2, 25);
        verify(image).setPixel(3, 35);
        verify(image).setPixel(4, 45);
    }
}
\end{verbatim}

Notice that:

\begin{aosaitemize}

\item
  We use the \texttt{MockitoJUnit} runner.
\item
  We mock \texttt{PApplet}, \texttt{IFAImage} (created for expressly
  this purpose), and \texttt{ImageColorHelper}.
\item
  Test methods are annotated with \texttt{@Test} \footnote{Method names
    in tests need not start with \texttt{test} as of JUnit 4 but habits
    are hard to break.}. If you want to ignore a test (e.g.~whilst
  debugging) you can add the annotation \texttt{@Ignore}.
\item
  In \texttt{setup()}, we create the pixel array and have the mock image
  always return it.
\item
  Helper methods make it easier to set expectations for reoccurring
  tasks (e.g. \texttt{setHsbValuesForPixel()},
  \texttt{setRgbValuesForPixel()}.)
\end{aosaitemize}

\aosasectii{Image State and Associated
Tests}\label{image-state-and-associated-tests}

\texttt{ImageState} holds the current ``state'' of the image - the image
itself, and the settings and filters that will be applied. We'll omit
the full implementation of ImageState here, but we'll show how it can be
tested. You can the source repository for this project to see the
implementation details of \texttt{ImageState}.

\begin{verbatim}
package com.catehuston.imagefilter.model;

import processing.core.PApplet;
import com.catehuston.imagefilter.color.ColorHelper;

public class ImageState {

    enum ColorMode {
        COLOR_FILTER,
        SHOW_DOMINANT_HUE,
        HIDE_DOMINANT_HUE
    }

    private final ColorHelper colorHelper;
    private IFAImage image;
    private String filepath;

    public static final int INITIAL_HUE_TOLERANCE = 5;

    ColorMode colorModeState = ColorMode.COLOR_FILTER;
    int blueFilter = 0;
    int greenFilter = 0;
    int hueTolerance = 0;
    int redFilter = 0;

    public ImageState(ColorHelper colorHelper) {
        this.colorHelper = colorHelper;
        image = new IFAImage();
        hueTolerance = INITIAL_HUE_TOLERANCE;
    }
    /* ... getters & setters */
    public void updateImage(PApplet applet, int hueRange, int rgbColorRange, 
            int imageMax) { ... }

    public void processKeyPress(char key, int inc, int rgbColorRange,
            int hueIncrement, int hueRange) { ... }

    public void setUpImage(PApplet applet, int imageMax) { ... }

    public void resetImage(PApplet applet, int imageMax) { ... }

    // For testing purposes only.
    protected void set(IFAImage image, ColorMode colorModeState,
            int redFilter, int greenFilter, int blueFilter, int hueTolerance) { ... }
}
\end{verbatim}

Here we can test that the appropriate actions happen for the given
state, that fields are incremented and decremented appropriately.

\begin{verbatim}
package com.catehuston.imagefilter.model;

/* ... Imports omitted ... */

@RunWith(MockitoJUnitRunner.class)
public class ImageStateTest {

    @Mock PApplet applet;
    @Mock ColorHelper colorHelper;
    @Mock IFAImage image;

    private ImageState imageState;

    @Before public void setUp() throws Exception {
        imageState = new ImageState(colorHelper);
    }

    private void assertState(ColorMode colorMode, int redFilter,
            int greenFilter, int blueFilter, int hueTolerance) {
        assertEquals(colorMode, imageState.getColorMode());
        assertEquals(redFilter, imageState.redFilter());
        assertEquals(greenFilter, imageState.greenFilter());
        assertEquals(blueFilter, imageState.blueFilter());
        assertEquals(hueTolerance, imageState.hueTolerance());
    }

    @Test public void testUpdateImageDominantHueHidden() {
        imageState.setFilepath("filepath");
        imageState.set(image, ColorMode.HIDE_DOMINANT_HUE, 5, 10, 15, 10);

        imageState.updateImage(applet, 100, 100, 500);

        verify(image).update(applet, "filepath");
        verify(colorHelper).processImageForHue(applet, image, 100, 10, false);
        verify(colorHelper).applyColorFilter(applet, image, 5, 10, 15, 100);
        verify(image).updatePixels();
    }

    @Test public void testUpdateDominantHueShowing() {
        imageState.setFilepath("filepath");
        imageState.set(image, ColorMode.SHOW_DOMINANT_HUE, 5, 10, 15, 10);

        imageState.updateImage(applet, 100, 100, 500);

        verify(image).update(applet, "filepath");
        verify(colorHelper).processImageForHue(applet, image, 100, 10, true);
        verify(colorHelper).applyColorFilter(applet, image, 5, 10, 15, 100);
        verify(image).updatePixels();
    }

    @Test public void testUpdateRGBOnly() {
        imageState.setFilepath("filepath");
        imageState.set(image, ColorMode.COLOR_FILTER, 5, 10, 15, 10);

        imageState.updateImage(applet, 100, 100, 500);

        verify(image).update(applet, "filepath");
        verify(colorHelper, never()).processImageForHue(any(PApplet.class), 
                any(IFAImage.class), anyInt(), anyInt(), anyBoolean());
        verify(colorHelper).applyColorFilter(applet, image, 5, 10, 15, 100);
        verify(image).updatePixels();
    }

    @Test public void testKeyPress() {
        imageState.processKeyPress('r', 5, 100, 2, 200);
        assertState(ColorMode.COLOR_FILTER, 5, 0, 0, 5);

        imageState.processKeyPress('e', 5, 100, 2, 200);
        assertState(ColorMode.COLOR_FILTER, 0, 0, 0, 5);

        imageState.processKeyPress('g', 5, 100, 2, 200);
        assertState(ColorMode.COLOR_FILTER, 0, 5, 0, 5);

        imageState.processKeyPress('f', 5, 100, 2, 200);
        assertState(ColorMode.COLOR_FILTER, 0, 0, 0, 5);

        imageState.processKeyPress('b', 5, 100, 2, 200);
        assertState(ColorMode.COLOR_FILTER, 0, 0, 5, 5);

        imageState.processKeyPress('v', 5, 100, 2, 200);
        assertState(ColorMode.COLOR_FILTER, 0, 0, 0, 5);

        imageState.processKeyPress('h', 5, 100, 2, 200);
        assertState(ColorMode.HIDE_DOMINANT_HUE, 0, 0, 0, 5);

        imageState.processKeyPress('i', 5, 100, 2, 200);
        assertState(ColorMode.HIDE_DOMINANT_HUE, 0, 0, 0, 7);

        imageState.processKeyPress('u', 5, 100, 2, 200);
        assertState(ColorMode.HIDE_DOMINANT_HUE, 0, 0, 0, 5);

        imageState.processKeyPress('h', 5, 100, 2, 200);
        assertState(ColorMode.COLOR_FILTER, 0, 0, 0, 5);

        imageState.processKeyPress('s', 5, 100, 2, 200);
        assertState(ColorMode.SHOW_DOMINANT_HUE, 0, 0, 0, 5);

        imageState.processKeyPress('s', 5, 100, 2, 200);
        assertState(ColorMode.COLOR_FILTER, 0, 0, 0, 5);

        // Random key should do nothing.
        imageState.processKeyPress('z', 5, 100, 2, 200);
        assertState(ColorMode.COLOR_FILTER, 0, 0, 0, 5);
    }

    @Test public void testSave() {
        imageState.set(image, ColorMode.SHOW_DOMINANT_HUE, 5, 10, 15, 10);
        imageState.setFilepath("filepath");
        imageState.processKeyPress('w', 5, 100, 2, 200);

        verify(image).save("filepath-new.png");
    }

    @Test public void testSetupImageLandscape() {
        imageState.set(image, ColorMode.SHOW_DOMINANT_HUE, 5, 10, 15, 10);
        when(image.getWidth()).thenReturn(20);
        when(image.getHeight()).thenReturn(8);
        imageState.setUpImage(applet, 10);
        verify(image).update(applet, null);
        verify(image).resize(10, 4);
    }

    @Test public void testSetupImagePortrait() {
        imageState.set(image, ColorMode.SHOW_DOMINANT_HUE, 5, 10, 15, 10);
        when(image.getWidth()).thenReturn(8);
        when(image.getHeight()).thenReturn(20);
        imageState.setUpImage(applet, 10);
        verify(image).update(applet, null);
        verify(image).resize(4, 10);
    }

    @Test public void testResetImage() {
        imageState.set(image, ColorMode.SHOW_DOMINANT_HUE, 5, 10, 15, 10);
        imageState.resetImage(applet, 10);
        assertState(ColorMode.COLOR_FILTER, 0, 0, 0, 5);
    }
}
\end{verbatim}

Notice that:

\begin{aosaitemize}

\item
  We exposed a protected initialization method \texttt{set} for testing
  that helps us quickly get the system under test into a specific state
\item
  We mock \texttt{PApplet}, \texttt{ColorHelper}, and \texttt{IFAImage}
  (created expressly for this purpose).
\item
  This time we use a helper method (\texttt{assertState()} to simplify
  asserting the state of the image.
\end{aosaitemize}

\aosasectiii{Measuring Test Coverage}\label{measuring-test-coverage}

I use
\href{http://www.eclemma.org/installation.html\#marketplace}{EclEmma} to
measure test coverage within Eclipse. It can be installed from the
Eclipse marketplace.

Overall for the app we have 81\% test coverage, with none of
\texttt{ImageFilterApp} covered, 94.8\% for \texttt{ImageState}, and
100\% for \texttt{ColorHelper}.

\aosasectii{ImageFilterApp}\label{imagefilterapp}

This is where everything is tied together, but we want as little as
possible there. The App is hard to unit test (much of it is layout), but
because we've pushed so much of the app's functionality into our own
tested classes, we're able to assure ourselves that the important parts
are working as intended.

We set the size of the app, and do the layout (these things are verified
by running the app and making sure it looks OK - no matter how good test
coverage this step should not be skipped!)

\begin{verbatim}
package com.catehuston.imagefilter.app;

import java.io.File;

import processing.core.PApplet;

import com.catehuston.imagefilter.color.ColorHelper;
import com.catehuston.imagefilter.color.PixelColorHelper;
import com.catehuston.imagefilter.model.ImageState;

@SuppressWarnings("serial")
public class ImageFilterApp extends PApplet {

    static final String INSTRUCTIONS = "...";

    static final int FILTER_HEIGHT = 2;
    static final int FILTER_INCREMENT = 5;
    static final int HUE_INCREMENT = 2;
    static final int HUE_RANGE = 100;
    static final int IMAGE_MAX = 640;
    static final int RGB_COLOR_RANGE = 100;
    static final int SIDE_BAR_PADDING = 10;
    static final int SIDE_BAR_WIDTH = RGB_COLOR_RANGE + 2 * SIDE_BAR_PADDING + 50;

    private ImageState imageState;

    boolean redrawImage = true;

    @Override
    public void setup() {
        noLoop();
        imageState = new ImageState(new ColorHelper(new PixelColorHelper()));

        // Set up the view.
        size(IMAGE_MAX + SIDE_BAR_WIDTH, IMAGE_MAX);
        background(0);

        chooseFile();
    }

    @Override
    public void draw() {
        // Draw image.
        if (imageState.image().image() != null && redrawImage) {
            background(0);
            drawImage();
        }

        colorMode(RGB, RGB_COLOR_RANGE);
        fill(0);
        rect(IMAGE_MAX, 0, SIDE_BAR_WIDTH, IMAGE_MAX);
        stroke(RGB_COLOR_RANGE);
        line(IMAGE_MAX, 0, IMAGE_MAX, IMAGE_MAX);

        // Draw red line
        int x = IMAGE_MAX + SIDE_BAR_PADDING;
        int y = 2 * SIDE_BAR_PADDING;
        stroke(RGB_COLOR_RANGE, 0, 0);
        line(x, y, x + RGB_COLOR_RANGE, y);
        line(x + imageState.redFilter(), y - FILTER_HEIGHT,
                x + imageState.redFilter(), y + FILTER_HEIGHT);

        // Draw green line
        y += 2 * SIDE_BAR_PADDING;
        stroke(0, RGB_COLOR_RANGE, 0);
        line(x, y, x + RGB_COLOR_RANGE, y);
        line(x + imageState.greenFilter(), y - FILTER_HEIGHT,
                x + imageState.greenFilter(), y + FILTER_HEIGHT);

        // Draw blue line
        y += 2 * SIDE_BAR_PADDING;
        stroke(0, 0, RGB_COLOR_RANGE);
        line(x, y, x + RGB_COLOR_RANGE, y);
        line(x + imageState.blueFilter(), y - FILTER_HEIGHT,
                x + imageState.blueFilter(), y + FILTER_HEIGHT);

        // Draw white line.
        y += 2 * SIDE_BAR_PADDING;
        stroke(HUE_RANGE);
        line(x, y, x + 100, y);
        line(x + imageState.hueTolerance(), y - FILTER_HEIGHT,
                x + imageState.hueTolerance(), y + FILTER_HEIGHT);

        y += 4 * SIDE_BAR_PADDING;
        fill(RGB_COLOR_RANGE);
        text(INSTRUCTIONS, x, y);

        updatePixels();
    }

    // Callback for selectInput(), has to be public to be found.
    public void fileSelected(File file) {
        if (file == null) {
            println("User hit cancel.");
        } else {
            imageState.setFilepath(file.getAbsolutePath());
            imageState.setUpImage(this, IMAGE_MAX);
            redrawImage = true;
            redraw();
        }
    }

    private void drawImage() {
        imageMode(CENTER);
        imageState.updateImage(this, HUE_RANGE, RGB_COLOR_RANGE, IMAGE_MAX);
        image(imageState.image().image(), IMAGE_MAX/2, IMAGE_MAX/2, 
                imageState.image().getWidth(), imageState.image().getHeight());
        redrawImage = false;
    }

    @Override
    public void keyPressed() {
        switch(key) {
        case 'c':
            chooseFile();
            break;
        case 'p':
            redrawImage = true;
            break;
        case ' ':
            imageState.resetImage(this, IMAGE_MAX);
            redrawImage = true;
            break;
        }
        imageState.processKeyPress(key, FILTER_INCREMENT, RGB_COLOR_RANGE, 
                HUE_INCREMENT, HUE_RANGE);
        redraw();
    }

    private void chooseFile() {
        // Choose the file.
        selectInput("Select a file to process:", "fileSelected");
    }
}
\end{verbatim}

Notice that:

\begin{aosaitemize}

\item
  Our implementation extends \texttt{PApplet}.
\item
  Most work is done in \texttt{ImageState}.
\item
  \texttt{fileSelected()} is the callback for \texttt{selectInput()}.
\item
  \texttt{static final} constants are defined up at the top.
\end{aosaitemize}

\aosasecti{The Value of Prototyping}\label{the-value-of-prototyping}

In real world programming, we spend a lot of time on productionisation
work. Making things look just so. Making them fail over. Maintaining
that 99.9\% uptime. We spend more time hunting down corner cases than
refining algorithms.

These constraints and requirements are important for our users. However
there's also a space for freeing ourselves from them to play, and
explore.

Eventually, I decided to port this to a native mobile app. Processing
has an Android library, but as many mobile developers do, I opted to go
iOS first. I had years of iOS experience, although I'd done little with
CoreGraphics, but I don't think even if I had had this idea initially, I
would have been able to build it straight away on iOS. The platform
forced me to operate in the RGB colorspace, and made it hard to extract
the pixels from the image (hello, C). Memory and waiting was a major
risk factor. There were exhilarating moments, when it worked for the
first time. When it first ran on my device\ldots{} without crashing.
When I optimized memory usage by 66\% and cut seconds off the runtime.
And there were large periods of time locked away in a dark room, cursing
intermittently.

Because I had my prototype, I could explain to my business partner and
our designer what I was thinking and what the app would do. It meant I
deeply understood how it would work, and it was just a question of
making it work nicely on this other platform. I knew what I was aiming
for, so at the end of a long day shut away fighting with it and feeling
like I had little to show for it I kept going\ldots{} and hit an
exhilarating moment and milestone the following morning.

So, how do you extract the dominant color from an image? There's an app
for that: \href{showandhide.com}{Show \& Hide}.

\end{aosachapter}
